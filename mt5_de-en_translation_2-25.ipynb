{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFA_cq0WEBWS"
   },
   "source": [
    "## **Model Fine-tuning** (Notebook sourced from translation notebook [here](https://huggingface.co/docs/transformers/notebooks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6zcdt7BpLJE"
   },
   "source": [
    "Enable logging with Weights and Biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jcuv1RmQpEt7"
   },
   "outputs": [],
   "source": [
    "wb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P0kUE65nK_9p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "work_dir = os.getcwd()\n",
    "if work_dir == '/content':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  os.chdir('drive/MyDrive/github_repos/XLdefgen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If running this on Colab, uncomment the following cell to install requisite packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MOsHUjgdIrIW"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers sacrebleu sentencepiece wandb\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EmnzArcYCFS8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/wildeb1/anaconda3/envs/XLdefgen/lib/python3.8/site-packages/wandb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrandonwilde\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=XLdefgen\n"
     ]
    }
   ],
   "source": [
    "if wb:\n",
    "  import wandb\n",
    "  print(wandb.__path__)\n",
    "  wandb.login()\n",
    "  %env WANDB_PROJECT=XLdefgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JL5wd9Z4u1U"
   },
   "source": [
    "If storing model on HF Model Hub, uncomment the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pIymjpij4u1V"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "A script version of this notebook to fine-tune the model in a distributed fashion using multiple GPUs or TPUs is available [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "Specify model checkpoint to load (from HF Model Hub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UlraLg9K4u1Y"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/mt5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt16 (/home/wildeb1/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data_path = \"codwoe_data.csv\"\n",
    "\n",
    "# class csvDataset(Dataset):\n",
    "\n",
    "#     def __init__(self,file_name):\n",
    "#         self.data_df = pd.read_csv(file_name)\n",
    "#         self.data_dict = data_df.to_dict(orient='index')\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "  \n",
    "#     def __getitem__(self,idx):\n",
    "#         import numbers\n",
    "#         if isinstance(idx, numbers.Integral):  # item is an integer\n",
    "#             idx = [idx]\n",
    "#         elif isinstance(idx, slice):  # item is a slice\n",
    "#             idx = list(range(idx.start or 0, idx.stop or len(self), idx.step or 1))\n",
    "#         else:  # invalid index type\n",
    "#             raise TypeError('{cls} indices must be integers or slices, not {idx}'.format(\n",
    "#                 cls=type(self).__name__,\n",
    "#                 idx=type(idx).__name__,\n",
    "#             ))\n",
    "\n",
    "#         return [self.data_dict[i] for i in idx]\n",
    "\n",
    "# codwoe_data = csvDataset(data_path)\n",
    "\n",
    "# raw_datasets = datasets.load_from_disk(\"de-en_wmt16_tokd\")\n",
    "raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'de': 'Die Premierminister Indiens und Japans trafen sich in Tokio.',\n",
       "  'en': 'India and Japan prime ministers meet in Tokyo'},\n",
       " {'de': 'Indiens neuer Premierminister Narendra Modi trifft bei seinem ersten wichtigen Auslandsbesuch seit seinem Wahlsieg im Mai seinen japanischen Amtskollegen Shinzo Abe in Toko, um wirtschaftliche und sicherheitspolitische Beziehungen zu besprechen.',\n",
       "  'en': \"India's new prime minister, Narendra Modi, is meeting his Japanese counterpart, Shinzo Abe, in Tokyo to discuss economic and security ties, on his first major foreign visit since winning May's election.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['validation']['translation'][:2]\n",
    "# codwoe_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function shows some examples picked randomly from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def show_random_elements(dataset, num_examples=5):\n",
    "#     assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "#     picks = []\n",
    "#     for _ in range(num_examples):\n",
    "#         pick = random.randint(0, len(dataset)-1)\n",
    "#         while pick in picks:\n",
    "#             pick = random.randint(0, len(dataset)-1)\n",
    "#         picks.append(pick)\n",
    "    \n",
    "#     df = pd.DataFrame(dataset[picks])\n",
    "#     for column, typ in dataset.features.items():\n",
    "#         if isinstance(typ, datasets.ClassLabel):\n",
    "#             df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "#     display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7"
   },
   "outputs": [],
   "source": [
    "# show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "Demonstration of the metric in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6XN1Rq0aIrJC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [4, 2, 0, 0],\n",
       " 'totals': [4, 2, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "Model-specific tokenizer adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zqn3ZeBl4u1k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs will include prefix!\n"
     ]
    }
   ],
   "source": [
    "if \"t5\" in model_checkpoint:\n",
    "#     prefix = \"translate German to English: \"\n",
    "    prefix = \"\"\n",
    "    print(\"Inputs will include prefix!\")\n",
    "else:\n",
    "    prefix = \"\"\n",
    "    print(\"Inputs will not include prefix!\")\n",
    "\n",
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"de-DE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzohepNVHEuL"
   },
   "source": [
    "Create preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 64\n",
    "max_target_length = 64\n",
    "source_lang = \"de\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1xBe3UiHTkp"
   },
   "source": [
    "Specify whether reduced dataset should be passed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sii3lf3ThnPM"
   },
   "outputs": [],
   "source": [
    "trim_datasets = True\n",
    "train_size = 10000\n",
    "eval_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8eOtIBLHgaB"
   },
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uAr_iWrLurIK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/wildeb1/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-4574fe47268cd3fd.arrow\n",
      "Loading cached shuffled indices for dataset at /home/wildeb1/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-bf1487bfb5cd2cad.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d7f239f8e34098b6c1a7e7924cca8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b258bd50494168965ba9731214477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets trimmed and tokenized.\n"
     ]
    }
   ],
   "source": [
    "if trim_datasets:\n",
    "  small_train_dataset = raw_datasets[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "  small_eval_dataset = raw_datasets[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "  raw_datasets_trim = datasets.DatasetDict({'train': small_train_dataset, 'validation': small_eval_dataset})\n",
    "  tokenized_datasets = raw_datasets_trim.map(preprocess_function, batched=True)\n",
    "  print(\"Datasets trimmed and tokenized.\")\n",
    "else:\n",
    "  tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "  print(\"Raw datasets tokenized.\")\n",
    "\n",
    "del raw_datasets #to clear memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "The results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). 🤗 Datasets warns you when it uses cached files, but you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TlqNaB8jIrJW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Specify batch size and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "learning_rate = 2e-4\n",
    "optim = 'adamw_hf'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "if wb:\n",
    "  report = \"wandb\"\n",
    "else:\n",
    "  report = \"none\"\n",
    "train_k = int(train_size/1000)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    # f\"drive/MyDrive/{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    # f\"XLdefgen-{source_lang}-to-{target_lang}\",\n",
    "    f\"XLd-trans-{source_lang}2{target_lang}-tr{train_k}k-b{batch_size}-lr{learning_rate}-{optim}\", #output directory\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "#     optim=optim,\n",
    "    adafactor=False,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3, #max num of checkpoints to keep\n",
    "    num_train_epochs=15,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,         #mixed precision (acceleration) - doesn't work well with t5 models\n",
    "    push_to_hub=False,  #push to HF Model Hub\n",
    "    report_to=report,   #for data logging\n",
    "#     run_name='Run_continued',     #for data logging\n",
    "    ignore_data_skip=False,   #if true and loading from checkpoint, this will start at beginning of dataset rather than where left off\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,  #defaults to true unless 'loss' is metric for best model\n",
    "    prediction_loss_only=False, #save space by not storing predictions for metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vtlfP4mLAtV"
   },
   "source": [
    "Add data collator to pad inputs and labels to max length for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-HycQdUC4u1o"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ylg71wOsLYgv"
   },
   "source": [
    "Post-processing and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#   '''Example for logging multiple metrics'''\n",
    "#     metric1 = load_metric(\"precision\")\n",
    "#     metric2 = load_metric(\"recall\")\n",
    "    \n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     precision = metric1.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "#     recall = metric2.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "#     return {\"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Instantiate Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "y3wZsCwxw0w2"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import math\n",
    "from transformers.debug_utils import DebugOption\n",
    "from transformers.trainer_utils import speed_metrics\n",
    "\n",
    "class PPLTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"\n",
    "    Just adapting Trainer to also log perplexity\n",
    "    \"\"\"\n",
    "    def evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "        max_length: Optional[int] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "    ) -> Dict[str, float]:\n",
    "                \n",
    "        # memory metrics - must set up as early as possible\n",
    "        self._memory_tracker.start()\n",
    "        \n",
    "        self._max_length = max_length if max_length is not None else self.args.generation_max_length\n",
    "        self._num_beams = num_beams if num_beams is not None else self.args.generation_num_beams\n",
    "\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        start_time = time.time()\n",
    "\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        output = eval_loop(\n",
    "            eval_dataloader,\n",
    "            description=\"Evaluation\",\n",
    "            # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "            # self.args.prediction_loss_only\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "        \n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        output.metrics.update(\n",
    "            {'eval_perplexity': round(math.exp(output.metrics['eval_loss']),4)}\n",
    "        )\n",
    "\n",
    "        self.log(output.metrics)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n",
    "\n",
    "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "\n",
    "        return output.metrics\n",
    "    \n",
    "    \n",
    "trainer = PPLTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "Train/fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z4xOvCQ3k1EY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/brandonwilde/XLdefgen/runs/2win1zgq\" target=\"_blank\">XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf</a></strong> to <a href=\"https://wandb.ai/brandonwilde/XLdefgen\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 24.315126419067383,\n",
       " 'eval_bleu': 0.0298,\n",
       " 'eval_gen_len': 2.66,\n",
       " 'eval_runtime': 4.7994,\n",
       " 'eval_samples_per_second': 20.836,\n",
       " 'eval_steps_per_second': 10.418,\n",
       " 'eval_perplexity': 36301555156.1729}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 75000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75000' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75000/75000 8:32:22, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.903800</td>\n",
       "      <td>3.202065</td>\n",
       "      <td>1.817800</td>\n",
       "      <td>16.820000</td>\n",
       "      <td>24.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.162000</td>\n",
       "      <td>3.013126</td>\n",
       "      <td>3.602000</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>20.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.977500</td>\n",
       "      <td>2.942097</td>\n",
       "      <td>4.058100</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>18.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.746000</td>\n",
       "      <td>2.851445</td>\n",
       "      <td>4.523000</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>17.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.698300</td>\n",
       "      <td>2.797816</td>\n",
       "      <td>5.407300</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>16.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.606300</td>\n",
       "      <td>2.720426</td>\n",
       "      <td>5.015700</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>15.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.559000</td>\n",
       "      <td>2.706430</td>\n",
       "      <td>5.989900</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>14.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.485200</td>\n",
       "      <td>2.673352</td>\n",
       "      <td>5.723500</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>14.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.476900</td>\n",
       "      <td>2.624709</td>\n",
       "      <td>5.968200</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>13.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.446700</td>\n",
       "      <td>2.618497</td>\n",
       "      <td>5.848900</td>\n",
       "      <td>17.560000</td>\n",
       "      <td>13.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.173200</td>\n",
       "      <td>2.608955</td>\n",
       "      <td>6.370700</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>13.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.144600</td>\n",
       "      <td>2.592904</td>\n",
       "      <td>6.282600</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>13.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.092700</td>\n",
       "      <td>2.577417</td>\n",
       "      <td>5.778000</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>13.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.092600</td>\n",
       "      <td>2.557143</td>\n",
       "      <td>5.500900</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.041000</td>\n",
       "      <td>2.538491</td>\n",
       "      <td>6.274100</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>12.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.070600</td>\n",
       "      <td>2.520765</td>\n",
       "      <td>5.586900</td>\n",
       "      <td>16.940000</td>\n",
       "      <td>12.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.014500</td>\n",
       "      <td>2.558723</td>\n",
       "      <td>5.601500</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>12.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.083500</td>\n",
       "      <td>2.549900</td>\n",
       "      <td>6.609000</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>12.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.016200</td>\n",
       "      <td>2.519713</td>\n",
       "      <td>5.777800</td>\n",
       "      <td>17.640000</td>\n",
       "      <td>12.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.001000</td>\n",
       "      <td>2.528007</td>\n",
       "      <td>6.119700</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>12.528500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.816100</td>\n",
       "      <td>2.509803</td>\n",
       "      <td>7.273600</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>12.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.753400</td>\n",
       "      <td>2.495579</td>\n",
       "      <td>6.884100</td>\n",
       "      <td>17.560000</td>\n",
       "      <td>12.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.776100</td>\n",
       "      <td>2.480984</td>\n",
       "      <td>6.950400</td>\n",
       "      <td>17.170000</td>\n",
       "      <td>11.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.851700</td>\n",
       "      <td>2.486086</td>\n",
       "      <td>6.290700</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>12.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.768900</td>\n",
       "      <td>2.488210</td>\n",
       "      <td>5.983400</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>12.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.757100</td>\n",
       "      <td>2.483066</td>\n",
       "      <td>6.444700</td>\n",
       "      <td>17.570000</td>\n",
       "      <td>11.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.744600</td>\n",
       "      <td>2.478389</td>\n",
       "      <td>6.181000</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>11.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.758700</td>\n",
       "      <td>2.500241</td>\n",
       "      <td>8.005500</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>12.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.737500</td>\n",
       "      <td>2.491396</td>\n",
       "      <td>7.432500</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>12.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.797500</td>\n",
       "      <td>2.462994</td>\n",
       "      <td>7.630900</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>11.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.528400</td>\n",
       "      <td>2.479299</td>\n",
       "      <td>6.372400</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>11.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.548500</td>\n",
       "      <td>2.488923</td>\n",
       "      <td>7.171400</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>12.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.551000</td>\n",
       "      <td>2.493787</td>\n",
       "      <td>6.844800</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>12.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.611500</td>\n",
       "      <td>2.469756</td>\n",
       "      <td>5.818900</td>\n",
       "      <td>17.510000</td>\n",
       "      <td>11.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.610300</td>\n",
       "      <td>2.466571</td>\n",
       "      <td>7.057500</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.582500</td>\n",
       "      <td>2.454173</td>\n",
       "      <td>6.496800</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>11.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.603000</td>\n",
       "      <td>2.464492</td>\n",
       "      <td>6.903300</td>\n",
       "      <td>17.060000</td>\n",
       "      <td>11.757500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.582200</td>\n",
       "      <td>2.444666</td>\n",
       "      <td>7.279200</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>11.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>2.450373</td>\n",
       "      <td>6.744400</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>11.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.575900</td>\n",
       "      <td>2.445172</td>\n",
       "      <td>7.183800</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>11.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.358600</td>\n",
       "      <td>2.462697</td>\n",
       "      <td>7.373500</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>11.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.446500</td>\n",
       "      <td>2.469364</td>\n",
       "      <td>7.505200</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>11.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.420200</td>\n",
       "      <td>2.481913</td>\n",
       "      <td>7.297900</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>11.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.397900</td>\n",
       "      <td>2.475992</td>\n",
       "      <td>6.348600</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>11.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.366800</td>\n",
       "      <td>2.474751</td>\n",
       "      <td>7.414800</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>11.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.371700</td>\n",
       "      <td>2.461490</td>\n",
       "      <td>6.258100</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>11.722300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.424500</td>\n",
       "      <td>2.457544</td>\n",
       "      <td>6.717800</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>11.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.454100</td>\n",
       "      <td>2.446449</td>\n",
       "      <td>6.238400</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>11.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.424400</td>\n",
       "      <td>2.428881</td>\n",
       "      <td>7.632500</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>11.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.429700</td>\n",
       "      <td>2.416293</td>\n",
       "      <td>7.577100</td>\n",
       "      <td>17.440000</td>\n",
       "      <td>11.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.236100</td>\n",
       "      <td>2.468407</td>\n",
       "      <td>7.781200</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>11.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.292000</td>\n",
       "      <td>2.446933</td>\n",
       "      <td>8.864700</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>11.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.238900</td>\n",
       "      <td>2.446099</td>\n",
       "      <td>7.499400</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>11.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>2.436331</td>\n",
       "      <td>7.926100</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>11.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.281900</td>\n",
       "      <td>2.439548</td>\n",
       "      <td>7.961300</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>11.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.323200</td>\n",
       "      <td>2.447484</td>\n",
       "      <td>7.326300</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>11.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.454803</td>\n",
       "      <td>6.932200</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.308800</td>\n",
       "      <td>2.449771</td>\n",
       "      <td>7.389700</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>11.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.336000</td>\n",
       "      <td>2.436009</td>\n",
       "      <td>7.740500</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>11.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.239800</td>\n",
       "      <td>2.441901</td>\n",
       "      <td>7.233200</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>11.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.114800</td>\n",
       "      <td>2.457712</td>\n",
       "      <td>7.157100</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>11.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.148200</td>\n",
       "      <td>2.451984</td>\n",
       "      <td>7.807200</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.150700</td>\n",
       "      <td>2.450674</td>\n",
       "      <td>7.170600</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>11.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.186800</td>\n",
       "      <td>2.458955</td>\n",
       "      <td>7.942900</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>11.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.146900</td>\n",
       "      <td>2.452045</td>\n",
       "      <td>7.770200</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>11.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.135800</td>\n",
       "      <td>2.441482</td>\n",
       "      <td>7.761500</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>11.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.152900</td>\n",
       "      <td>2.444855</td>\n",
       "      <td>7.398500</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>11.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.228700</td>\n",
       "      <td>2.445451</td>\n",
       "      <td>7.941000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.218900</td>\n",
       "      <td>2.414630</td>\n",
       "      <td>6.941700</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>11.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.131200</td>\n",
       "      <td>2.437914</td>\n",
       "      <td>7.667200</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>11.449100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.034400</td>\n",
       "      <td>2.457093</td>\n",
       "      <td>7.297500</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>11.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.988400</td>\n",
       "      <td>2.470148</td>\n",
       "      <td>8.194700</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>11.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.040200</td>\n",
       "      <td>2.441570</td>\n",
       "      <td>7.737700</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>11.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.097900</td>\n",
       "      <td>2.460391</td>\n",
       "      <td>7.484200</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>11.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.064300</td>\n",
       "      <td>2.456995</td>\n",
       "      <td>7.822600</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>11.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.085200</td>\n",
       "      <td>2.444523</td>\n",
       "      <td>8.047600</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>11.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>2.034200</td>\n",
       "      <td>2.443688</td>\n",
       "      <td>8.175800</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>11.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.095800</td>\n",
       "      <td>2.450627</td>\n",
       "      <td>7.401800</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>11.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>2.046600</td>\n",
       "      <td>2.445453</td>\n",
       "      <td>7.212500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>11.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.074400</td>\n",
       "      <td>2.444277</td>\n",
       "      <td>7.914200</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>11.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>1.958700</td>\n",
       "      <td>2.467405</td>\n",
       "      <td>7.403500</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.947900</td>\n",
       "      <td>2.479013</td>\n",
       "      <td>7.852700</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>11.929500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>1.993900</td>\n",
       "      <td>2.460731</td>\n",
       "      <td>7.812200</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>11.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.982700</td>\n",
       "      <td>2.482060</td>\n",
       "      <td>7.267500</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>11.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.954300</td>\n",
       "      <td>2.447549</td>\n",
       "      <td>7.134600</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>11.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.968400</td>\n",
       "      <td>2.450819</td>\n",
       "      <td>7.385500</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>11.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>1.955500</td>\n",
       "      <td>2.459332</td>\n",
       "      <td>7.584300</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>11.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.939200</td>\n",
       "      <td>2.445344</td>\n",
       "      <td>7.409500</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>11.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>1.983900</td>\n",
       "      <td>2.456797</td>\n",
       "      <td>6.913500</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>11.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.960900</td>\n",
       "      <td>2.454295</td>\n",
       "      <td>7.289800</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>11.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>1.881400</td>\n",
       "      <td>2.475581</td>\n",
       "      <td>7.825400</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>11.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.821100</td>\n",
       "      <td>2.500107</td>\n",
       "      <td>7.655500</td>\n",
       "      <td>17.130000</td>\n",
       "      <td>12.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>1.854500</td>\n",
       "      <td>2.489295</td>\n",
       "      <td>7.221000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>12.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.887300</td>\n",
       "      <td>2.503687</td>\n",
       "      <td>7.381100</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>12.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.921600</td>\n",
       "      <td>2.469618</td>\n",
       "      <td>5.866600</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.927100</td>\n",
       "      <td>2.472225</td>\n",
       "      <td>7.427100</td>\n",
       "      <td>17.130000</td>\n",
       "      <td>11.848800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>1.901200</td>\n",
       "      <td>2.469635</td>\n",
       "      <td>8.367100</td>\n",
       "      <td>17.170000</td>\n",
       "      <td>11.818100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>1.855100</td>\n",
       "      <td>2.481635</td>\n",
       "      <td>8.096000</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>11.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>1.860200</td>\n",
       "      <td>2.470214</td>\n",
       "      <td>7.951500</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>11.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.929800</td>\n",
       "      <td>2.463237</td>\n",
       "      <td>8.322400</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>1.795900</td>\n",
       "      <td>2.473325</td>\n",
       "      <td>8.881900</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>11.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.785500</td>\n",
       "      <td>2.489645</td>\n",
       "      <td>8.601000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>12.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>1.816600</td>\n",
       "      <td>2.476438</td>\n",
       "      <td>8.074900</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>11.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.772200</td>\n",
       "      <td>2.478811</td>\n",
       "      <td>7.995500</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.830100</td>\n",
       "      <td>2.487041</td>\n",
       "      <td>7.927700</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>12.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.821100</td>\n",
       "      <td>2.488285</td>\n",
       "      <td>8.206200</td>\n",
       "      <td>17.170000</td>\n",
       "      <td>12.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>1.824400</td>\n",
       "      <td>2.479340</td>\n",
       "      <td>8.086100</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>11.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.769400</td>\n",
       "      <td>2.487095</td>\n",
       "      <td>8.082500</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>12.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>1.852500</td>\n",
       "      <td>2.486420</td>\n",
       "      <td>7.823600</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>12.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.855900</td>\n",
       "      <td>2.464365</td>\n",
       "      <td>7.751800</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>11.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>1.746700</td>\n",
       "      <td>2.493140</td>\n",
       "      <td>7.628400</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>12.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.739900</td>\n",
       "      <td>2.500139</td>\n",
       "      <td>7.964000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>12.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>1.723400</td>\n",
       "      <td>2.496951</td>\n",
       "      <td>7.459000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>12.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.751200</td>\n",
       "      <td>2.503166</td>\n",
       "      <td>8.035400</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>12.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.743400</td>\n",
       "      <td>2.507776</td>\n",
       "      <td>7.739700</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>12.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.747300</td>\n",
       "      <td>2.498801</td>\n",
       "      <td>8.163200</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>12.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>1.802700</td>\n",
       "      <td>2.490154</td>\n",
       "      <td>7.614500</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>12.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.795700</td>\n",
       "      <td>2.492576</td>\n",
       "      <td>7.538600</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>12.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>1.797000</td>\n",
       "      <td>2.490005</td>\n",
       "      <td>8.186100</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>12.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.766000</td>\n",
       "      <td>2.491136</td>\n",
       "      <td>7.774800</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>1.709300</td>\n",
       "      <td>2.508486</td>\n",
       "      <td>8.020100</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>12.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.691600</td>\n",
       "      <td>2.515051</td>\n",
       "      <td>7.022100</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>12.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>1.724300</td>\n",
       "      <td>2.517088</td>\n",
       "      <td>7.533200</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>12.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>1.721500</td>\n",
       "      <td>2.510848</td>\n",
       "      <td>8.114400</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>12.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>1.633900</td>\n",
       "      <td>2.521082</td>\n",
       "      <td>7.715200</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>12.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>1.720500</td>\n",
       "      <td>2.505216</td>\n",
       "      <td>7.792700</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>12.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>1.703700</td>\n",
       "      <td>2.507246</td>\n",
       "      <td>7.748300</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>12.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>1.726700</td>\n",
       "      <td>2.513123</td>\n",
       "      <td>7.441400</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>12.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>1.708800</td>\n",
       "      <td>2.512420</td>\n",
       "      <td>7.378100</td>\n",
       "      <td>17.040000</td>\n",
       "      <td>12.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.748200</td>\n",
       "      <td>2.502439</td>\n",
       "      <td>7.760800</td>\n",
       "      <td>17.170000</td>\n",
       "      <td>12.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>1.625200</td>\n",
       "      <td>2.519712</td>\n",
       "      <td>7.670700</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>12.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.665600</td>\n",
       "      <td>2.516283</td>\n",
       "      <td>7.662600</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>12.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>1.687400</td>\n",
       "      <td>2.519430</td>\n",
       "      <td>7.723600</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>12.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.663800</td>\n",
       "      <td>2.519889</td>\n",
       "      <td>7.557100</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>12.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>1.726900</td>\n",
       "      <td>2.515194</td>\n",
       "      <td>7.998600</td>\n",
       "      <td>17.040000</td>\n",
       "      <td>12.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>1.682500</td>\n",
       "      <td>2.511272</td>\n",
       "      <td>7.919700</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>12.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>1.647600</td>\n",
       "      <td>2.504005</td>\n",
       "      <td>7.765200</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>12.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>1.661600</td>\n",
       "      <td>2.514972</td>\n",
       "      <td>7.995400</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>12.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>1.663700</td>\n",
       "      <td>2.518161</td>\n",
       "      <td>7.903300</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>12.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.707700</td>\n",
       "      <td>2.513715</td>\n",
       "      <td>7.552700</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>12.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>1.633000</td>\n",
       "      <td>2.520093</td>\n",
       "      <td>7.956900</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>12.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>1.644800</td>\n",
       "      <td>2.521294</td>\n",
       "      <td>7.560400</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>12.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>1.646300</td>\n",
       "      <td>2.520677</td>\n",
       "      <td>7.547000</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>12.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>1.629000</td>\n",
       "      <td>2.522428</td>\n",
       "      <td>8.084100</td>\n",
       "      <td>17.030000</td>\n",
       "      <td>12.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.619400</td>\n",
       "      <td>2.522059</td>\n",
       "      <td>8.060400</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>12.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>1.639500</td>\n",
       "      <td>2.521714</td>\n",
       "      <td>7.822400</td>\n",
       "      <td>17.060000</td>\n",
       "      <td>12.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>1.615600</td>\n",
       "      <td>2.523662</td>\n",
       "      <td>7.876800</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>12.474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>1.643600</td>\n",
       "      <td>2.524164</td>\n",
       "      <td>7.972500</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>12.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>1.642500</td>\n",
       "      <td>2.522799</td>\n",
       "      <td>7.972500</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>12.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.642800</td>\n",
       "      <td>2.522957</td>\n",
       "      <td>7.975800</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>12.465400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-9500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-10500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-11500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-12500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-13500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-16500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-15000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-17500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-18500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-20500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-21500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-22500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-23500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-24500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-26500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-27500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-28500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-29500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-30500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-31500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-32500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-25000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-33500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-35500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-36500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-37500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-38500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-39500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-40500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-41500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-42500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-43500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-44500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-45500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-46500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-47500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-48500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-49500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-50500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-51500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-52500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-53500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-54500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-55500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-56500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-57500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-58500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-59500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-60500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-61500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-62500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-63500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-64500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-65500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-66500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-67500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-68500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-69500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-70500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-71500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-72500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74500\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74500/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-73500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-75000\n",
      "Configuration saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-75000/config.json\n",
      "Model weights saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-75000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-75000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-74000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf/checkpoint-34500 (score: 2.41463041305542).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75000, training_loss=2.2516505916341147, metrics={'train_runtime': 30742.555, 'train_samples_per_second': 4.879, 'train_steps_per_second': 2.44, 'total_flos': 1.37861361297408e+16, 'train_loss': 2.2516505916341147, 'epoch': 15.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() #to free up space\n",
    "# if wb:\n",
    "#   wandb.init(resume=True) #this is performed by the trainer\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-U-C97b1LkWO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 82515... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▅▆▅▅▆▅▆▆▆▆▆▇▇▆▆▇▆▇▇▇▆▆▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>eval/gen_len</td><td>▁▃▅▆▆█▄▆▅▅▅▅▆▆▃▅▅▅▅▅▆▅▄▄▄▄▅▄▅▅▄▄▃▄▄▃▄▃▃▅</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/perplexity</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▂▃█▃▃▄▄▂▂▃▃▂▃▄▂▂▃▂▂▁▃▃▃▃▇▄▂▂▂▁▃▂▄▂▂▂▂▄▃▄</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▁▅▆▅▄▇▇▆▆▇▆▄▇▇▆▆▇█▆▆▆▆▂▅▇▇▇█▅▇▄▇▇▇▇▄▆▅</td></tr><tr><td>eval/steps_per_second</td><td>▇▆▁▅▆▅▄▇▇▆▆▇▆▄▇▇▆▆▇█▆▆▆▆▂▅▇▇▇█▅▇▄▇▇▇▇▄▆▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>6.9417</td></tr><tr><td>eval/gen_len</td><td>17.29</td></tr><tr><td>eval/loss</td><td>2.41463</td></tr><tr><td>eval/perplexity</td><td>11.1856</td></tr><tr><td>eval/runtime</td><td>16.6017</td></tr><tr><td>eval/samples_per_second</td><td>6.023</td></tr><tr><td>eval/steps_per_second</td><td>3.012</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>75000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6428</td></tr><tr><td>train/total_flos</td><td>1.37861361297408e+16</td></tr><tr><td>train/train_loss</td><td>2.25165</td></tr><tr><td>train/train_runtime</td><td>30742.555</td></tr><tr><td>train/train_samples_per_second</td><td>4.879</td></tr><tr><td>train/train_steps_per_second</td><td>2.44</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">XLd-trans-de2en-tr10k-b2-lr0.0002-adamw_hf</strong>: <a href=\"https://wandb.ai/brandonwilde/XLdefgen/runs/2win1zgq\" target=\"_blank\">https://wandb.ai/brandonwilde/XLdefgen/runs/2win1zgq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220226_163154-2win1zgq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "if wb:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RBwGyYrN084"
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr95bq5YMirp"
   },
   "source": [
    "Test model predictive capacity with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fz827skL-FKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  336,  9070,   259, 42822,   514,  1037, 64712, 10990,     1]])\n",
      "tensor([[  336,  9070,   259, 42822,   514,  1037, 64712, 10990,     1]])\n",
      "tensor([[  4824,  65941,    259,  69474, 176055,  18156,    278,    749,    326,\n",
      "           2786,    259,    263,  71632,    272,    447, 114328,   4573,      1]])\n",
      "tensor([[ 1089, 32397,   348,  2504,   398, 29671,   265,  1230,   390, 40481,\n",
      "           260,     1]])\n",
      "\n",
      "Greedy Output:\n",
      "The professor cannot carry the matter.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Beam Output:\n",
      "['The professor cannot carry out the matter.', 'The professor cannot carry the matter.', 'The professor cannot assume the matter.']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prefix + 'I enjoy walking with my cute dog', return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = tokenizer(prefix + 'I enjoy walking with my cute dog', return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = tokenizer(prefix + 'Ich gehe gern spazierien mit meinem süßen Hündchen', return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = tokenizer(prefix + \"Die Professorin kann die Sache nicht betragen.\", return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "greedy_output = model.generate(input_ids)\n",
    "print(\"\\nGreedy Output:\")\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True, min_length=5))\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "print(\"\\n\" + 100 * '-' + \"\\n\\nBeam Output:\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZGyR1ZG4u1r"
   },
   "source": [
    "Push Model to HF Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zZID0xEz4u1r"
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mt5_de-en_translation_2-5.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "14f3cr9x_NFH3ijBAiW8eX_4tXDSJtYLf",
     "timestamp": 1643210084469
    },
    {
     "file_id": "1jecC75oqiCXVSDCkOz_GOJlEhRx-c6Q_",
     "timestamp": 1642539157370
    },
    {
     "file_id": "1J3ttZqUQHCa4BwtdHuC2qDIZLJVnCsk0",
     "timestamp": 1642536077715
    },
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/examples/translation.ipynb",
     "timestamp": 1642535483080
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
