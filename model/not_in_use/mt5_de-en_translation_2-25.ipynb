{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFA_cq0WEBWS"
   },
   "source": [
    "## **Model Fine-tuning** (Notebook sourced from translation notebook [here](https://huggingface.co/docs/transformers/notebooks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6zcdt7BpLJE"
   },
   "source": [
    "Enable logging with Weights and Biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jcuv1RmQpEt7"
   },
   "outputs": [],
   "source": [
    "wb = True  # Enable WeightsAndBiases tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P0kUE65nK_9p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "work_dir = os.getcwd()\n",
    "if work_dir == '/content':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  os.chdir('drive/MyDrive/github_repos/XLdefgen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If running this on Colab, uncomment the following cell to install requisite packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MOsHUjgdIrIW"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers sacrebleu sentencepiece wandb\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EmnzArcYCFS8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/wildeb1/anaconda3/envs/XLdefgen/lib/python3.8/site-packages/wandb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrandonwilde\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=XLdefgen\n"
     ]
    }
   ],
   "source": [
    "if wb:\n",
    "  import wandb\n",
    "  print(wandb.__path__)\n",
    "  wandb.login()\n",
    "  %env WANDB_PROJECT=XLdefgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JL5wd9Z4u1U"
   },
   "source": [
    "If storing model on HF Model Hub, uncomment the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pIymjpij4u1V"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "A script version of this notebook to fine-tune the model in a distributed fashion using multiple GPUs or TPUs is available [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "Specify model checkpoint to load (from HF Model Hub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UlraLg9K4u1Y"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/mt5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt16 (/home/wildeb1/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data_path = \"codwoe_data.csv\"\n",
    "\n",
    "# class csvDataset(Dataset):\n",
    "\n",
    "#     def __init__(self,file_name):\n",
    "#         self.data_df = pd.read_csv(file_name)\n",
    "#         self.data_dict = data_df.to_dict(orient='index')\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "  \n",
    "#     def __getitem__(self,idx):\n",
    "#         import numbers\n",
    "#         if isinstance(idx, numbers.Integral):  # item is an integer\n",
    "#             idx = [idx]\n",
    "#         elif isinstance(idx, slice):  # item is a slice\n",
    "#             idx = list(range(idx.start or 0, idx.stop or len(self), idx.step or 1))\n",
    "#         else:  # invalid index type\n",
    "#             raise TypeError('{cls} indices must be integers or slices, not {idx}'.format(\n",
    "#                 cls=type(self).__name__,\n",
    "#                 idx=type(idx).__name__,\n",
    "#             ))\n",
    "\n",
    "#         return [self.data_dict[i] for i in idx]\n",
    "\n",
    "# codwoe_data = csvDataset(data_path)\n",
    "\n",
    "# raw_datasets = datasets.load_from_disk(\"de-en_wmt16_tokd\")\n",
    "raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'de': 'Die Premierminister Indiens und Japans trafen sich in Tokio.',\n",
       "  'en': 'India and Japan prime ministers meet in Tokyo'},\n",
       " {'de': 'Indiens neuer Premierminister Narendra Modi trifft bei seinem ersten wichtigen Auslandsbesuch seit seinem Wahlsieg im Mai seinen japanischen Amtskollegen Shinzo Abe in Toko, um wirtschaftliche und sicherheitspolitische Beziehungen zu besprechen.',\n",
       "  'en': \"India's new prime minister, Narendra Modi, is meeting his Japanese counterpart, Shinzo Abe, in Tokyo to discuss economic and security ties, on his first major foreign visit since winning May's election.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['validation']['translation'][:2]\n",
    "# codwoe_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function shows some examples picked randomly from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def show_random_elements(dataset, num_examples=5):\n",
    "#     assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "#     picks = []\n",
    "#     for _ in range(num_examples):\n",
    "#         pick = random.randint(0, len(dataset)-1)\n",
    "#         while pick in picks:\n",
    "#             pick = random.randint(0, len(dataset)-1)\n",
    "#         picks.append(pick)\n",
    "    \n",
    "#     df = pd.DataFrame(dataset[picks])\n",
    "#     for column, typ in dataset.features.items():\n",
    "#         if isinstance(typ, datasets.ClassLabel):\n",
    "#             df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "#     display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7"
   },
   "outputs": [],
   "source": [
    "# show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "Demonstration of the metric in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6XN1Rq0aIrJC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [4, 2, 0, 0],\n",
       " 'totals': [4, 2, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "Model-specific tokenizer adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zqn3ZeBl4u1k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs will include prefix!\n"
     ]
    }
   ],
   "source": [
    "if \"t5\" in model_checkpoint:\n",
    "    prefix = \"translate German to English: \"\n",
    "#     prefix = \"\"\n",
    "    print(\"Inputs will include prefix!\")\n",
    "else:\n",
    "    prefix = \"\"\n",
    "    print(\"Inputs will not include prefix!\")\n",
    "\n",
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"de-DE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzohepNVHEuL"
   },
   "source": [
    "Create preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 64\n",
    "max_target_length = 64\n",
    "source_lang = \"de\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True).input_ids\n",
    "    \n",
    "    labels_with_ignore_index = []\n",
    "    for labels_example in labels:\n",
    "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
    "        labels_with_ignore_index.append(labels_example)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1xBe3UiHTkp"
   },
   "source": [
    "Specify whether reduced dataset should be passed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sii3lf3ThnPM"
   },
   "outputs": [],
   "source": [
    "trim_datasets = True\n",
    "train_size = 10000\n",
    "eval_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8eOtIBLHgaB"
   },
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uAr_iWrLurIK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/wildeb1/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-4574fe47268cd3fd.arrow\n",
      "Loading cached shuffled indices for dataset at /home/wildeb1/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-bf1487bfb5cd2cad.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a080f7d9db34f29b9ac8833f19331e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636acd4eec6b4e71887186cb4820bcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets trimmed and tokenized.\n"
     ]
    }
   ],
   "source": [
    "if trim_datasets:\n",
    "    small_train_dataset = raw_datasets[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "    small_eval_dataset = raw_datasets[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "    raw_datasets_trim = datasets.DatasetDict({'train': small_train_dataset, 'validation': small_eval_dataset})\n",
    "    tokenized_datasets = raw_datasets_trim.map(preprocess_function, batched=True)\n",
    "    print(\"Datasets trimmed and tokenized.\")\n",
    "else:\n",
    "    tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "    print(\"Raw datasets tokenized.\")\n",
    "\n",
    "del raw_datasets #to clear memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "The results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). 🤗 Datasets warns you when it uses cached files, but you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TlqNaB8jIrJW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Specify batch size and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "learning_rate = 2e-4\n",
    "optim = 'adamw_hf'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "if wb:\n",
    "  report = \"wandb\"\n",
    "else:\n",
    "  report = \"none\"\n",
    "train_k = int(train_size/1000)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    # f\"drive/MyDrive/{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    # f\"XLdefgen-{source_lang}-to-{target_lang}\",\n",
    "#     f\"XLd-trans-{source_lang}2{target_lang}-tr{train_k}k-b{batch_size}-lr{learning_rate}-{optim}\", #output directory\n",
    "    \"XLd-trans-fixed_padding\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "#     optim=optim,\n",
    "    adafactor=False,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3, #max num of checkpoints to keep\n",
    "    num_train_epochs=15,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,         #mixed precision (acceleration) - doesn't work well with t5 models\n",
    "    push_to_hub=False,  #push to HF Model Hub\n",
    "    report_to=report,   #for data logging\n",
    "#     run_name='Run_continued',     #for data logging\n",
    "    ignore_data_skip=False,   #if true and loading from checkpoint, this will start at beginning of dataset rather than where left off\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,  #defaults to true unless 'loss' is metric for best model\n",
    "    prediction_loss_only=False, #save space by not storing predictions for metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vtlfP4mLAtV"
   },
   "source": [
    "Add data collator to pad inputs and labels to max length for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-HycQdUC4u1o"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ylg71wOsLYgv"
   },
   "source": [
    "Post-processing and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#   '''Example for logging multiple metrics'''\n",
    "#     metric1 = load_metric(\"precision\")\n",
    "#     metric2 = load_metric(\"recall\")\n",
    "    \n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     precision = metric1.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "#     recall = metric2.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "#     return {\"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Instantiate Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "y3wZsCwxw0w2"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import math\n",
    "from transformers.debug_utils import DebugOption\n",
    "from transformers.trainer_utils import speed_metrics\n",
    "\n",
    "class PPLTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"\n",
    "    Just adapting Trainer to also log perplexity\n",
    "    \"\"\"\n",
    "    def evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "        max_length: Optional[int] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "    ) -> Dict[str, float]:\n",
    "                \n",
    "        # memory metrics - must set up as early as possible\n",
    "        self._memory_tracker.start()\n",
    "        \n",
    "        self._max_length = max_length if max_length is not None else self.args.generation_max_length\n",
    "        self._num_beams = num_beams if num_beams is not None else self.args.generation_num_beams\n",
    "\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        start_time = time.time()\n",
    "\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        output = eval_loop(\n",
    "            eval_dataloader,\n",
    "            description=\"Evaluation\",\n",
    "            # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "            # self.args.prediction_loss_only\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "        \n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        output.metrics.update(\n",
    "            {'eval_perplexity': round(math.exp(output.metrics['eval_loss']),4)}\n",
    "        )\n",
    "\n",
    "        self.log(output.metrics)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n",
    "\n",
    "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "\n",
    "        return output.metrics\n",
    "    \n",
    "    \n",
    "trainer = PPLTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "Train/fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z4xOvCQ3k1EY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/brandonwilde/XLdefgen/runs/3u1ov3ce\" target=\"_blank\">XLd-trans-fixed_padding</a></strong> to <a href=\"https://wandb.ai/brandonwilde/XLdefgen\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 22.351808547973633,\n",
       " 'eval_bleu': 0.1885,\n",
       " 'eval_gen_len': 2.38,\n",
       " 'eval_runtime': 4.586,\n",
       " 'eval_samples_per_second': 21.805,\n",
       " 'eval_steps_per_second': 10.903,\n",
       " 'eval_perplexity': 5096442315.0861}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 75000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75000' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75000/75000 8:50:25, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.303800</td>\n",
       "      <td>3.156665</td>\n",
       "      <td>2.711200</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>23.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.137000</td>\n",
       "      <td>2.942436</td>\n",
       "      <td>3.267600</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>18.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.960400</td>\n",
       "      <td>2.908991</td>\n",
       "      <td>4.101900</td>\n",
       "      <td>16.930000</td>\n",
       "      <td>18.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.734500</td>\n",
       "      <td>2.828471</td>\n",
       "      <td>4.570700</td>\n",
       "      <td>16.980000</td>\n",
       "      <td>16.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.660400</td>\n",
       "      <td>2.772385</td>\n",
       "      <td>5.070100</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>15.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.588500</td>\n",
       "      <td>2.700920</td>\n",
       "      <td>5.984900</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>14.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.547700</td>\n",
       "      <td>2.695486</td>\n",
       "      <td>4.813600</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>14.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.485500</td>\n",
       "      <td>2.663942</td>\n",
       "      <td>4.607700</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>14.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.447900</td>\n",
       "      <td>2.618433</td>\n",
       "      <td>5.706800</td>\n",
       "      <td>16.970000</td>\n",
       "      <td>13.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.440900</td>\n",
       "      <td>2.592508</td>\n",
       "      <td>5.673400</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>13.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.150200</td>\n",
       "      <td>2.586921</td>\n",
       "      <td>6.571600</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>13.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.145800</td>\n",
       "      <td>2.569449</td>\n",
       "      <td>6.574300</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>13.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.080400</td>\n",
       "      <td>2.554392</td>\n",
       "      <td>5.727300</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>12.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.072300</td>\n",
       "      <td>2.554106</td>\n",
       "      <td>5.907900</td>\n",
       "      <td>16.960000</td>\n",
       "      <td>12.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.051300</td>\n",
       "      <td>2.537005</td>\n",
       "      <td>5.885200</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>12.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.067600</td>\n",
       "      <td>2.523405</td>\n",
       "      <td>5.377200</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>12.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.018200</td>\n",
       "      <td>2.537081</td>\n",
       "      <td>5.869000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>12.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.075400</td>\n",
       "      <td>2.539225</td>\n",
       "      <td>6.091900</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>12.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.002100</td>\n",
       "      <td>2.522093</td>\n",
       "      <td>5.762400</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>12.454600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.002300</td>\n",
       "      <td>2.518049</td>\n",
       "      <td>6.819200</td>\n",
       "      <td>17.460000</td>\n",
       "      <td>12.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.820900</td>\n",
       "      <td>2.518143</td>\n",
       "      <td>6.670100</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>12.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.750100</td>\n",
       "      <td>2.500920</td>\n",
       "      <td>7.003200</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.761900</td>\n",
       "      <td>2.512214</td>\n",
       "      <td>6.530500</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>12.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.843500</td>\n",
       "      <td>2.492454</td>\n",
       "      <td>6.928400</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>12.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.764500</td>\n",
       "      <td>2.510563</td>\n",
       "      <td>6.592000</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>12.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.762000</td>\n",
       "      <td>2.492130</td>\n",
       "      <td>6.086000</td>\n",
       "      <td>17.130000</td>\n",
       "      <td>12.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.740400</td>\n",
       "      <td>2.505427</td>\n",
       "      <td>6.097200</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>12.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.745900</td>\n",
       "      <td>2.479271</td>\n",
       "      <td>6.348400</td>\n",
       "      <td>16.980000</td>\n",
       "      <td>11.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.744900</td>\n",
       "      <td>2.478576</td>\n",
       "      <td>7.117000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>11.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.778400</td>\n",
       "      <td>2.455471</td>\n",
       "      <td>7.206500</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>11.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.521500</td>\n",
       "      <td>2.465937</td>\n",
       "      <td>7.252300</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.541700</td>\n",
       "      <td>2.484178</td>\n",
       "      <td>7.083300</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>11.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.550100</td>\n",
       "      <td>2.478668</td>\n",
       "      <td>7.322800</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>11.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.618100</td>\n",
       "      <td>2.455379</td>\n",
       "      <td>7.393900</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>11.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.595200</td>\n",
       "      <td>2.459686</td>\n",
       "      <td>6.871600</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>11.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.573900</td>\n",
       "      <td>2.456779</td>\n",
       "      <td>7.105100</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>11.667200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.596100</td>\n",
       "      <td>2.462782</td>\n",
       "      <td>6.483900</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>11.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.581100</td>\n",
       "      <td>2.450871</td>\n",
       "      <td>7.270400</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>11.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.544900</td>\n",
       "      <td>2.457633</td>\n",
       "      <td>7.609900</td>\n",
       "      <td>17.470000</td>\n",
       "      <td>11.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.558900</td>\n",
       "      <td>2.437471</td>\n",
       "      <td>7.888400</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>11.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.358700</td>\n",
       "      <td>2.445007</td>\n",
       "      <td>8.143100</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>11.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.451900</td>\n",
       "      <td>2.472147</td>\n",
       "      <td>7.659900</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>11.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.415000</td>\n",
       "      <td>2.489053</td>\n",
       "      <td>8.126600</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>12.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.406200</td>\n",
       "      <td>2.467653</td>\n",
       "      <td>6.990000</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>11.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.379700</td>\n",
       "      <td>2.478809</td>\n",
       "      <td>7.540700</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>11.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.370700</td>\n",
       "      <td>2.451264</td>\n",
       "      <td>6.432200</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>11.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.429000</td>\n",
       "      <td>2.476429</td>\n",
       "      <td>7.581800</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>11.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.458800</td>\n",
       "      <td>2.443135</td>\n",
       "      <td>6.687200</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>11.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.423400</td>\n",
       "      <td>2.445168</td>\n",
       "      <td>7.490900</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>11.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.437000</td>\n",
       "      <td>2.426652</td>\n",
       "      <td>7.640500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>11.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.252000</td>\n",
       "      <td>2.466784</td>\n",
       "      <td>7.780500</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>11.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.289400</td>\n",
       "      <td>2.468721</td>\n",
       "      <td>7.985900</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>11.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.235700</td>\n",
       "      <td>2.468096</td>\n",
       "      <td>7.862700</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>11.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.276600</td>\n",
       "      <td>2.452215</td>\n",
       "      <td>6.708100</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>11.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.286000</td>\n",
       "      <td>2.481756</td>\n",
       "      <td>8.021200</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>11.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.321600</td>\n",
       "      <td>2.482313</td>\n",
       "      <td>7.501000</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>11.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.238000</td>\n",
       "      <td>2.475101</td>\n",
       "      <td>7.629600</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>11.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.301800</td>\n",
       "      <td>2.454766</td>\n",
       "      <td>6.631600</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>11.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.334700</td>\n",
       "      <td>2.444862</td>\n",
       "      <td>6.890600</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>11.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.241000</td>\n",
       "      <td>2.442574</td>\n",
       "      <td>7.439300</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.104300</td>\n",
       "      <td>2.470814</td>\n",
       "      <td>7.162000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>11.832100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>2.480104</td>\n",
       "      <td>7.405900</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>11.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.144200</td>\n",
       "      <td>2.472010</td>\n",
       "      <td>7.151000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>11.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.190700</td>\n",
       "      <td>2.475013</td>\n",
       "      <td>7.857300</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>11.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.150900</td>\n",
       "      <td>2.479940</td>\n",
       "      <td>7.661900</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>11.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.125800</td>\n",
       "      <td>2.481847</td>\n",
       "      <td>7.637600</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.149300</td>\n",
       "      <td>2.452082</td>\n",
       "      <td>7.380800</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>11.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.226300</td>\n",
       "      <td>2.469099</td>\n",
       "      <td>7.243100</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>11.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.208100</td>\n",
       "      <td>2.434500</td>\n",
       "      <td>7.295400</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>11.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.127300</td>\n",
       "      <td>2.452348</td>\n",
       "      <td>7.036800</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.026700</td>\n",
       "      <td>2.479564</td>\n",
       "      <td>6.797500</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>11.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.004400</td>\n",
       "      <td>2.479874</td>\n",
       "      <td>7.547200</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>11.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.037900</td>\n",
       "      <td>2.471910</td>\n",
       "      <td>7.210200</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.101600</td>\n",
       "      <td>2.480234</td>\n",
       "      <td>7.710800</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.070600</td>\n",
       "      <td>2.463961</td>\n",
       "      <td>7.134700</td>\n",
       "      <td>17.130000</td>\n",
       "      <td>11.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.087600</td>\n",
       "      <td>2.467948</td>\n",
       "      <td>6.672300</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>11.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>2.043400</td>\n",
       "      <td>2.467243</td>\n",
       "      <td>7.389700</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>11.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.100300</td>\n",
       "      <td>2.463077</td>\n",
       "      <td>6.291600</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>2.049700</td>\n",
       "      <td>2.487120</td>\n",
       "      <td>6.954400</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>12.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.056100</td>\n",
       "      <td>2.473906</td>\n",
       "      <td>6.567200</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>11.868700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>1.968300</td>\n",
       "      <td>2.492257</td>\n",
       "      <td>6.548800</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>12.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.946200</td>\n",
       "      <td>2.532185</td>\n",
       "      <td>6.862100</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>2.003300</td>\n",
       "      <td>2.500319</td>\n",
       "      <td>7.254700</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>12.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.995600</td>\n",
       "      <td>2.499821</td>\n",
       "      <td>7.163900</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>12.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.942800</td>\n",
       "      <td>2.486326</td>\n",
       "      <td>7.429400</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>12.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.958500</td>\n",
       "      <td>2.493776</td>\n",
       "      <td>7.149700</td>\n",
       "      <td>17.460000</td>\n",
       "      <td>12.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>1.940400</td>\n",
       "      <td>2.497922</td>\n",
       "      <td>6.621700</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>12.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.945700</td>\n",
       "      <td>2.478056</td>\n",
       "      <td>7.186700</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>11.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>2.007800</td>\n",
       "      <td>2.497239</td>\n",
       "      <td>7.350500</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.970000</td>\n",
       "      <td>2.468855</td>\n",
       "      <td>6.882800</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>11.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>1.874100</td>\n",
       "      <td>2.518538</td>\n",
       "      <td>6.665500</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>12.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.832500</td>\n",
       "      <td>2.528339</td>\n",
       "      <td>6.901800</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>12.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>1.867400</td>\n",
       "      <td>2.517824</td>\n",
       "      <td>6.997800</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>12.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.888800</td>\n",
       "      <td>2.534162</td>\n",
       "      <td>8.606700</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.928900</td>\n",
       "      <td>2.494266</td>\n",
       "      <td>7.535300</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.930400</td>\n",
       "      <td>2.505001</td>\n",
       "      <td>7.107700</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>12.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>1.913200</td>\n",
       "      <td>2.503368</td>\n",
       "      <td>7.110300</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>12.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>1.844700</td>\n",
       "      <td>2.498157</td>\n",
       "      <td>7.441600</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>1.859700</td>\n",
       "      <td>2.508804</td>\n",
       "      <td>7.656100</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>12.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.932800</td>\n",
       "      <td>2.502691</td>\n",
       "      <td>7.916800</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>1.796700</td>\n",
       "      <td>2.530373</td>\n",
       "      <td>7.970400</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.787600</td>\n",
       "      <td>2.527026</td>\n",
       "      <td>7.312700</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>12.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>1.822400</td>\n",
       "      <td>2.513963</td>\n",
       "      <td>7.737300</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>12.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.764500</td>\n",
       "      <td>2.519926</td>\n",
       "      <td>7.516200</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.837300</td>\n",
       "      <td>2.509291</td>\n",
       "      <td>6.602000</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>12.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.829800</td>\n",
       "      <td>2.515979</td>\n",
       "      <td>7.113600</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>12.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>1.824300</td>\n",
       "      <td>2.511223</td>\n",
       "      <td>7.040700</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>12.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.770600</td>\n",
       "      <td>2.508371</td>\n",
       "      <td>8.124400</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>1.855700</td>\n",
       "      <td>2.505470</td>\n",
       "      <td>7.487500</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.855800</td>\n",
       "      <td>2.512888</td>\n",
       "      <td>7.428900</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>12.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>1.749200</td>\n",
       "      <td>2.535375</td>\n",
       "      <td>7.220500</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.739100</td>\n",
       "      <td>2.555999</td>\n",
       "      <td>7.344100</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>12.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>1.714800</td>\n",
       "      <td>2.542194</td>\n",
       "      <td>7.695100</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.756600</td>\n",
       "      <td>2.531447</td>\n",
       "      <td>7.831400</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>12.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.758400</td>\n",
       "      <td>2.545093</td>\n",
       "      <td>7.553200</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>12.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.740100</td>\n",
       "      <td>2.540046</td>\n",
       "      <td>7.815800</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>12.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>1.795800</td>\n",
       "      <td>2.540663</td>\n",
       "      <td>7.487500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>12.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.790700</td>\n",
       "      <td>2.535055</td>\n",
       "      <td>7.691100</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>1.796500</td>\n",
       "      <td>2.534724</td>\n",
       "      <td>8.457700</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>12.612900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.761900</td>\n",
       "      <td>2.524743</td>\n",
       "      <td>8.603700</td>\n",
       "      <td>17.170000</td>\n",
       "      <td>12.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>1.716700</td>\n",
       "      <td>2.548824</td>\n",
       "      <td>7.995400</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>12.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.682000</td>\n",
       "      <td>2.552224</td>\n",
       "      <td>7.897900</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>12.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>1.728300</td>\n",
       "      <td>2.547331</td>\n",
       "      <td>8.408500</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>12.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>1.732300</td>\n",
       "      <td>2.537866</td>\n",
       "      <td>8.355000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>12.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>1.635100</td>\n",
       "      <td>2.552878</td>\n",
       "      <td>8.767500</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>12.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>1.730200</td>\n",
       "      <td>2.550090</td>\n",
       "      <td>8.631300</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>1.719600</td>\n",
       "      <td>2.544137</td>\n",
       "      <td>7.519100</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>12.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>1.717400</td>\n",
       "      <td>2.547547</td>\n",
       "      <td>8.138000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>12.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>1.714100</td>\n",
       "      <td>2.553490</td>\n",
       "      <td>8.467700</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>12.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.744000</td>\n",
       "      <td>2.544908</td>\n",
       "      <td>8.456700</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>1.650700</td>\n",
       "      <td>2.563809</td>\n",
       "      <td>8.134900</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>12.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.662800</td>\n",
       "      <td>2.564207</td>\n",
       "      <td>8.512000</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>12.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>1.695400</td>\n",
       "      <td>2.564626</td>\n",
       "      <td>7.724400</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>12.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.667500</td>\n",
       "      <td>2.562978</td>\n",
       "      <td>7.964400</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>12.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>1.725500</td>\n",
       "      <td>2.561154</td>\n",
       "      <td>7.538300</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>12.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>1.674600</td>\n",
       "      <td>2.557387</td>\n",
       "      <td>8.254800</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>1.648200</td>\n",
       "      <td>2.557646</td>\n",
       "      <td>8.204000</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>12.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>1.650600</td>\n",
       "      <td>2.564326</td>\n",
       "      <td>8.245500</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>12.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>1.654600</td>\n",
       "      <td>2.562321</td>\n",
       "      <td>7.796800</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>12.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.708300</td>\n",
       "      <td>2.557987</td>\n",
       "      <td>8.526900</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>12.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>1.642400</td>\n",
       "      <td>2.568002</td>\n",
       "      <td>8.128200</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>13.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>1.646100</td>\n",
       "      <td>2.570665</td>\n",
       "      <td>8.458100</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>13.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>1.639900</td>\n",
       "      <td>2.567419</td>\n",
       "      <td>8.223500</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>13.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>1.640800</td>\n",
       "      <td>2.569999</td>\n",
       "      <td>8.135900</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>13.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.620900</td>\n",
       "      <td>2.570665</td>\n",
       "      <td>8.649500</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>13.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>1.636800</td>\n",
       "      <td>2.569927</td>\n",
       "      <td>8.329700</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>13.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>1.607900</td>\n",
       "      <td>2.573859</td>\n",
       "      <td>8.305200</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>13.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>1.647900</td>\n",
       "      <td>2.570737</td>\n",
       "      <td>8.282800</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>13.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>1.636000</td>\n",
       "      <td>2.569471</td>\n",
       "      <td>8.277900</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>13.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.642500</td>\n",
       "      <td>2.570134</td>\n",
       "      <td>8.268200</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>13.067600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-500/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-1000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-1000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-1000/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-1500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-1500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-1500/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-2000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-2000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-2000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-2000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-2500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-2500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-2500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-2500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-3000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-3000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-3500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-3500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-3500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-3500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-4000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-4000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-4000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-4000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-4500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-4500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-4500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-5000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-5000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-5000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-5000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-5500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-5500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-5500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-5500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-5500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-6000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-6000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-6500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-6500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-6500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-6500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-7000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-7000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-7000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-7000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-7500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-7500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-8000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-8000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-8000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-8000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-8500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-8500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-8500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-8500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-9000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-9000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-9000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-9000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-9500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-9500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-9500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-9500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-10000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-10000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-10000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-10000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-10500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-10500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-10500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-10500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-11000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-11000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-11000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-11000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-9500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-11500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-11500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-11500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-11500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-12000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-12000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-12000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-12000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-10500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-12500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-12500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-12500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-12500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-11000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-13000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-13000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-13000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-13000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-11500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-13500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-13500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-13500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-13500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-14000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-14000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-14000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-14000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-12500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-14500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-14500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-14500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-14500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-13000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-15000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-15000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-15000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-15000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-13500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-15500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-15500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-15500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-15500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-14000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-16000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-16000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-16000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-16000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-16500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-16500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-16500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-16500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-15500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-17000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-17000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-17000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-17000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-15000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-17500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-17500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-17500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-17500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-16000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-18000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-18000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-18000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-18000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-16500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-18500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-18500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-18500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-18500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-17500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-19000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-19000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-19000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-19000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-17000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-19500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-19500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-19500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-19500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-18000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-20000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-20000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-20000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-20000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-18500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-20500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-20500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-20500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-20500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-21000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-21000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-21000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-21000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-19500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-21500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-21500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-21500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-21500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-20500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-22000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-22000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-22000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-22000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-21000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-22500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-22500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-22500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-22500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-21500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-23000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-23000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-23000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-23000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-22000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-23500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-23500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-23500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-23500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-22500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-24000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-24000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-24000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-24000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-23000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-24500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-24500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-24500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-24500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-23500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-25000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-25000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-25000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-25000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-20000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-25500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-25500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-25500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-25500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-24000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-26000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-26000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-26000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-24500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-26500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-26500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-26500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-26500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-25500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-27000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-27000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-27000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-27000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-26000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-27500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-27500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-27500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-27500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-26500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-28000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-28000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-28000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-28000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-27000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-28500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-28500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-28500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-28500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-27500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-29000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-29000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-29000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-29000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-28000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-29500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-29500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-29500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-29500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-28500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-30000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-30000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-30000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-30000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-29000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-30500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-30500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-30500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-30500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-29500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-31000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-31000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-31000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-31000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-30000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-31500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-31500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-31500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-31500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-30500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-32000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-32000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-32000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-32000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-31000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-32500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-32500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-32500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-32500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-31500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-33000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-33000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-33000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-33000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-32000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-33500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-33500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-33500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-33500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-32500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-34000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-34000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-34000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-34000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-33000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-34500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-34500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-34500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-34500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-33500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-35000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-35000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-35000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-35000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-34000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-35500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-35500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-35500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-35500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-34500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-36000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-36000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-36000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-36000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-36000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-35000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-36500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-36500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-36500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-36500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-35500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-37000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-37000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-37000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-37000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-36000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-37500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-37500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-37500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-37500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-36500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-38000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-38000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-38000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-38000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-37000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-38500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-38500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-38500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-38500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-37500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-39000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-39000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-39000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-39000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-38000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-39500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-39500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-39500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-39500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-38500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-40000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-40000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-40000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-40000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-39000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-40500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-40500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-40500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-40500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-39500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-41000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-41000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-41000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-41000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-41000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-40000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-41500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-41500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-41500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-41500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-40500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-42000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-42000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-42000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-42000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-41000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-42500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-42500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-42500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-42500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-41500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-43000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-43000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-43000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-43000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-42000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-43500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-43500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-43500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-43500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-42500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-44000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-44000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-44000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-44000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-43000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-44500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-44500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-44500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-44500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-43500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-45000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-45000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-45000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-45000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-44000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-45500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-45500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-45500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-45500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-44500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-46000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-46000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-46000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-46000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-45000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-46500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-46500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-46500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-46500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-45500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-47000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-47000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-47000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-47000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-46000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-47500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-47500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-47500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-47500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-46500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-48000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-48000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-48000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-48000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-47000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-48500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-48500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-48500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-48500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-47500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-49000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-49000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-49000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-49000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-48000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-49500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-49500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-49500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-49500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-48500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-50000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-50000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-50000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-50000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-49000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-50500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-50500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-50500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-50500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-49500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-51000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-51000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-51000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-51000/spiece.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-50000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-51500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-51500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-51500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-51500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-50500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-52000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-52000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-52000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-52000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-51000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-52500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-52500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-52500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-52500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-51500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-53000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-53000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-53000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-53000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-52000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-53500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-53500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-53500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-53500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-52500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-54000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-54000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-54000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-54000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-53000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-54500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-54500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-54500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-54500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-53500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-55000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-55000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-55000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-55000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-54000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-55500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-55500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-55500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-55500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-54500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-56000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-56000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-56000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-56000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-55000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-56500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-56500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-56500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-56500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-55500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-57000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-57000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-57000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-57000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-56000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-57500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-57500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-57500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-57500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-56500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-58000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-58000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-58000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-58000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-57000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-58500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-58500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-58500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-58500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-57500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-59000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-59000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-59000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-59000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-58000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-59500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-59500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-59500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-59500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-58500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-60000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-60000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-60000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-60000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-59000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-60500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-60500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-60500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-60500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-59500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-61000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-61000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-61000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-61000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-60000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-61500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-61500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-61500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-61500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-60500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-62000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-62000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-62000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-62000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-61000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-62500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-62500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-62500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-62500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-61500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-63000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-63000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-63000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-63000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-62000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-63500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-63500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-63500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-63500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-62500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-64000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-64000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-64000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-64000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-63000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-64500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-64500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-64500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-64500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-63500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-65000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-65000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-65000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-65000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-64000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-65500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-65500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-65500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-65500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-64500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-66000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-66000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-66000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-66000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-65000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-66500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-66500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-66500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-66500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-65500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-67000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-67000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-67000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-67000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-66000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-67500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-67500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-67500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-67500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-66500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-68000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-68000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-68000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-68000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-67000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-68500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-68500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-68500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-68500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-67500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-69000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-69000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-69000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-69000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-68000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-69500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-69500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-69500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-69500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-68500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-70000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-70000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-70000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-70000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-69000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-70500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-70500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-70500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-70500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-69500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-71000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-71000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-71000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-71000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-70000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-71500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-71500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-71500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-71500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-70500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-72000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-72000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-72000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-72000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-71000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-72500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-72500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-72500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-72500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-71500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-73000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-73000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-73000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-73000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-72000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-73500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-73500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-73500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-73500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-72500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-74000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-74000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-74000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-74000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-73000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-74500\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-74500/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-74500/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-74500/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-73500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to XLd-trans-fixed_padding/checkpoint-75000\n",
      "Configuration saved in XLd-trans-fixed_padding/checkpoint-75000/config.json\n",
      "Model weights saved in XLd-trans-fixed_padding/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in XLd-trans-fixed_padding/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in XLd-trans-fixed_padding/checkpoint-75000/special_tokens_map.json\n",
      "Copy vocab file to XLd-trans-fixed_padding/checkpoint-75000/spiece.model\n",
      "Deleting older checkpoint [XLd-trans-fixed_padding/checkpoint-74000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from XLd-trans-fixed_padding/checkpoint-25000 (score: 2.426652193069458).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75000, training_loss=2.2463020670572917, metrics={'train_runtime': 31825.6835, 'train_samples_per_second': 4.713, 'train_steps_per_second': 2.357, 'total_flos': 1.9828113408e+16, 'train_loss': 2.2463020670572917, 'epoch': 15.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() #to free up space\n",
    "# if wb:\n",
    "#   wandb.init(resume=True) #this is performed by the trainer\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-U-C97b1LkWO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 53318... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▄▆▅▅▆▅▇▆▇█▇▇▇▇▆▇▇▆▇▆▇▇▆▆▇▇▆▆▇██▇█▇▇██▇</td></tr><tr><td>eval/gen_len</td><td>▇▂▁▆▄▄▆▇▆▇█▆▆▇▆▇▇▆▆▆▇▆▇▅▇▆▅▇▇▅▆▅▆▇▅▆▆▆▇▇</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/perplexity</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▂▂▄▂▂▁▁▂▂▂▃▂▁▃▂▄▃▄▃▂▂▃▁▂█▂▁▂▄▄▂▄▃▃▃▃▄▂▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▅▇▇█▇▇▇▇▅▇█▆▇▄▅▅▆▇▇▆█▇▁▇█▇▅▅▇▄▆▅▆▅▅▇▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▅▇▇█▇▇▇▇▅▇█▆▇▄▅▅▆▇▇▆█▇▁▇█▇▅▅▇▄▆▅▆▅▅▇▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>7.6405</td></tr><tr><td>eval/gen_len</td><td>17.4</td></tr><tr><td>eval/loss</td><td>2.42665</td></tr><tr><td>eval/perplexity</td><td>11.3209</td></tr><tr><td>eval/runtime</td><td>15.797</td></tr><tr><td>eval/samples_per_second</td><td>6.33</td></tr><tr><td>eval/steps_per_second</td><td>3.165</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>75000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6425</td></tr><tr><td>train/total_flos</td><td>1.9828113408e+16</td></tr><tr><td>train/train_loss</td><td>2.2463</td></tr><tr><td>train/train_runtime</td><td>31825.6835</td></tr><tr><td>train/train_samples_per_second</td><td>4.713</td></tr><tr><td>train/train_steps_per_second</td><td>2.357</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">XLd-trans-fixed_padding</strong>: <a href=\"https://wandb.ai/brandonwilde/XLdefgen/runs/3u1ov3ce\" target=\"_blank\">https://wandb.ai/brandonwilde/XLdefgen/runs/3u1ov3ce</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220227_063912-3u1ov3ce/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "if wb:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RBwGyYrN084"
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr95bq5YMirp"
   },
   "source": [
    "Test model predictive capacity with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fz827skL-FKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37194, 20567,   288,  5413,   267,   336,  9070,   259, 42822,   514,\n",
      "          1037, 64712, 10990,     1]])\n",
      "tensor([[37194, 20567,   288,  5413,   267,   336,  9070,   259, 42822,   514,\n",
      "          1037, 64712, 10990,     1]])\n",
      "tensor([[ 37194,  20567,    288,   5413,    267,   4824,  65941,    259,  69474,\n",
      "         176055,  18156,    278,    749,    326,   2786,    259,    263,  71632,\n",
      "            272,    447, 114328,   4573,      1]])\n",
      "tensor([[37194, 20567,   288,  5413,   267,  1089, 32397,   348,  2504,   398,\n",
      "         29671,   265,  1230,   390, 40481,   260,     1]])\n",
      "\n",
      "Greedy Output:\n",
      "The professor cannot carry out the matter.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Beam Output:\n",
      "['The professor cannot carry out the matter.', 'The professor cannot carry out the case.', 'The professor cannot take the case.']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prefix + 'I enjoy walking with my cute dog', return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = tokenizer(prefix + 'I enjoy walking with my cute dog', return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = tokenizer(prefix + 'Ich gehe gern spazierien mit meinem süßen Hündchen', return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = tokenizer(prefix + \"Die Professorin kann die Sache nicht betragen.\", return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "greedy_output = model.generate(input_ids)\n",
    "print(\"\\nGreedy Output:\")\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True, min_length=5))\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "print(\"\\n\" + 100 * '-' + \"\\n\\nBeam Output:\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZGyR1ZG4u1r"
   },
   "source": [
    "Push Model to HF Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zZID0xEz4u1r"
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mt5_de-en_translation_2-5.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "14f3cr9x_NFH3ijBAiW8eX_4tXDSJtYLf",
     "timestamp": 1643210084469
    },
    {
     "file_id": "1jecC75oqiCXVSDCkOz_GOJlEhRx-c6Q_",
     "timestamp": 1642539157370
    },
    {
     "file_id": "1J3ttZqUQHCa4BwtdHuC2qDIZLJVnCsk0",
     "timestamp": 1642536077715
    },
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/examples/translation.ipynb",
     "timestamp": 1642535483080
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
