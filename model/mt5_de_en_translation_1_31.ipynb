{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Fine-tuning** (Notebook sourced from translation notebook [here](https://huggingface.co/docs/transformers/notebooks))"
      ],
      "metadata": {
        "id": "WFA_cq0WEBWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable logging with Weights and Biases:"
      ],
      "metadata": {
        "id": "-6zcdt7BpLJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wb = False"
      ],
      "metadata": {
        "id": "jcuv1RmQpEt7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "work_dir = os.getcwd()\n",
        "if work_dir == '/content':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  os.chdir('/content/drive/MyDrive/XLdefgen')"
      ],
      "metadata": {
        "id": "P0kUE65nK_9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If running this on Colab, uncomment the following cell to install requisite packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers sacrebleu sentencepiece wandb\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmnzArcYCFS8"
      },
      "outputs": [],
      "source": [
        "if wb:\n",
        "  import wandb\n",
        "  wandb.login()\n",
        "  %env WANDB_PROJECT=XLdefgen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JL5wd9Z4u1U"
      },
      "source": [
        "If storing model on HF Model Hub, uncomment the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIymjpij4u1V"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "A script version of this notebook to fine-tune the model in a distributed fashion using multiple GPUs or TPUs is available [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "Specify model checkpoint to load (from HF Model Hub)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlraLg9K4u1Y"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/mt5-small\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function shows some examples picked randomly from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "# import datasets\n",
        "# import random\n",
        "# import pandas as pd\n",
        "# from IPython.display import display, HTML\n",
        "\n",
        "# def show_random_elements(dataset, num_examples=5):\n",
        "#     assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "#     picks = []\n",
        "#     for _ in range(num_examples):\n",
        "#         pick = random.randint(0, len(dataset)-1)\n",
        "#         while pick in picks:\n",
        "#             pick = random.randint(0, len(dataset)-1)\n",
        "#         picks.append(pick)\n",
        "    \n",
        "#     df = pd.DataFrame(dataset[picks])\n",
        "#     for column, typ in dataset.features.items():\n",
        "#         if isinstance(typ, datasets.ClassLabel):\n",
        "#             df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "#     display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7"
      },
      "outputs": [],
      "source": [
        "# show_random_elements(raw_datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "Demonstration of the metric in use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XN1Rq0aIrJC"
      },
      "outputs": [],
      "source": [
        "fake_preds = [\"hello there\", \"general kenobi\"]\n",
        "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "Model-specific tokenizer adaptations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqn3ZeBl4u1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6b8896-b246-406d-8c9e-c94bc0fe065b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs will include prefix!\n"
          ]
        }
      ],
      "source": [
        "if \"t5\" in model_checkpoint:\n",
        "    prefix = \"translate English to German: \"\n",
        "    print(\"Inputs will include prefix!\")\n",
        "else:\n",
        "    prefix = \"\"\n",
        "    print(\"Inputs will not include prefix!\")\n",
        "\n",
        "if \"mbart\" in model_checkpoint:\n",
        "    tokenizer.src_lang = \"en-XX\"\n",
        "    tokenizer.tgt_lang = \"de-DE\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create preprocessing function"
      ],
      "metadata": {
        "id": "AzohepNVHEuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "source_lang = \"en\"\n",
        "target_lang = \"de\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify whether reduced dataset should be passed to model"
      ],
      "metadata": {
        "id": "s1xBe3UiHTkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sii3lf3ThnPM"
      },
      "outputs": [],
      "source": [
        "trim_datasets = True\n",
        "train_size = 10000\n",
        "eval_size = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data"
      ],
      "metadata": {
        "id": "r8eOtIBLHgaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAr_iWrLurIK"
      },
      "outputs": [],
      "source": [
        "if trim_datasets:\n",
        "  small_train_dataset = raw_datasets[\"train\"].shuffle(seed=42).select(range(train_size))\n",
        "  small_eval_dataset = raw_datasets[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
        "  raw_datasets_trim = datasets.DatasetDict({'train': small_train_dataset, 'validation': small_eval_dataset})\n",
        "  tokenized_datasets = raw_datasets_trim.map(preprocess_function, batched=True)\n",
        "  print(\"Datasets trimmed and tokenized.\")\n",
        "else:\n",
        "  tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "  print(\"Raw datasets tokenized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "The results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). ðŸ¤— Datasets warns you when it uses cached files, but you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "Specify batch size and training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9686f388-6586-4cfa-835d-8eabf9e2034a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using `logging_steps` to initialize `eval_steps` to 500\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "if wb:\n",
        "  report = \"wandb\"\n",
        "else:\n",
        "  report = \"none\"\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    # f\"drive/MyDrive/{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    # f\"drive/MyDrive/XLdefgen-{source_lang}-to-{target_lang}\",\n",
        "    f\"XLdefgen-trans-{source_lang}-to-{target_lang}-train{train_size}-bat{batch_size}\", #output directory\n",
        "    evaluation_strategy = \"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3, #max num of checkpoints to keep\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,         #mixed precision (acceleration) - doesn't work well with t5 models\n",
        "    push_to_hub=False,  #push to HF Model Hub\n",
        "    report_to=report,   #for data logging\n",
        "    ignore_data_skip=True   #if true and loading from checkpoint, this will start at beginning of dataset rather than where left off\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add data collator to pad inputs and labels to max length for each batch"
      ],
      "metadata": {
        "id": "3vtlfP4mLAtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HycQdUC4u1o"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-processing and compute metrics"
      ],
      "metadata": {
        "id": "Ylg71wOsLYgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Instantiate Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "Train/fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "d0a13584-8bd8-4397-f73f-a69a6caa3643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: translation.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   3/1250 00:00 < 15:20, 1.35 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-5361d1fa3256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 11.17 GiB total capacity; 9.60 GiB already allocated; 785.81 MiB free; 9.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache() #to free up space\n",
        "if wb:\n",
        "  wandb.init()\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "if wb:\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model testing"
      ],
      "metadata": {
        "id": "3RBwGyYrN084"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model predictive capacity with an example"
      ],
      "metadata": {
        "id": "Tr95bq5YMirp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz827skL-FKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0414357-b301-4cc4-c170-7178eeb25812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[37194,  5413,   288, 20567,   267,   336,  9070,   259, 42822,   514,\n",
            "          1037, 64712, 10990,     1]])\n",
            "tensor([[37194,  5413,   288, 20567,   267,   336,  9070,   259, 42822,   514,\n",
            "          1037, 64712, 10990,     1]])\n",
            "\n",
            "Greedy Output:\n",
            "FÃ¼r einen kleines Haustiere spielen!\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Beam Output:\n",
            "['ã€‚ã€Ž walking with I walking with  my cute sheeping.', 'ã€‚ã€Ž walking with I walking with  my cute sheeping, dass ich', 'ã€‚ã€Ž walking with I walking with  my cute sheeping and a ']\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer.encode(prefix + 'I enjoy walking with my cute dog', return_tensors='pt')\n",
        "print(input_ids)\n",
        "\n",
        "input_ids = tokenizer(prefix + 'I enjoy walking with my cute dog', return_tensors='pt').input_ids\n",
        "print(input_ids)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "greedy_output = model.generate(input_ids)\n",
        "print(\"\\nGreedy Output:\")\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True, min_length=5))\n",
        "\n",
        "outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
        "print(\"\\n\" + 100 * '-' + \"\\n\\nBeam Output:\")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZGyR1ZG4u1r"
      },
      "source": [
        "Push Model to HF Model Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZID0xEz4u1r"
      },
      "outputs": [],
      "source": [
        "# trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mt5_de-en_translation_1-31",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}